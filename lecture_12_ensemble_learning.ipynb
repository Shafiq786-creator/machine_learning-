{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b8d188",
   "metadata": {},
   "source": [
    "## LECTURE 12: Ensemble Learning \n",
    "\n",
    "## course: Awfera Machine Learning\n",
    "\n",
    "## Instructor: Dr. Shazia Saqib\n",
    "\n",
    "## Student: Muhammad Shafiq\n",
    "\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9a8730",
   "metadata": {},
   "source": [
    "# Introduction to Ensamble Learning\n",
    "\n",
    "### What is Ensemble Learning?\n",
    "Ensemble learning combines multiple weak learners to create a stronger model. it's like asking multiple experts for advice to make a better decision.\n",
    "\n",
    "### The power of collective wisdom:\n",
    "By leveraging diverse perspectives, ensemble method often outperform individual models in accuracy and robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8381228",
   "metadata": {},
   "source": [
    "## Types of Ensemble Methods:\n",
    "There are three main types of ensemble learning techniques: *Bagging*, *Boosting*, and *Stacking*. Each method has its distinct characteristics and applications.\n",
    "\n",
    "## 1. Bagging\n",
    " - **Definition**: Bagging, or Bootstrap Aggregating, involves training multiple models in parallel using different subsets of the data. The final prediction is made by combining the results from all models.\n",
    " - **Key Characteristics:**\n",
    "    - **Parallel Models**: Models are trained independently and simultaneously.\n",
    "    - **Variance Reduction**: Bagging helps in reducing variance by averaging multiple predictions, which makes it less sensitive to data fluctuations.\n",
    "## **Common Example**:\n",
    " Random Forest is a classic example of a bagging model. It combines multiple decision trees for classification or regression tasks.\n",
    " - **Application**: Bagging is particularly useful when you need to reduce the variance in the predictions, making it ideal for high-variance models such as decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142ecda7",
   "metadata": {},
   "source": [
    "## 2. Boosting\n",
    " - **Definition**: Boosting is an ensemble technique where models are trained sequentially. Each new model focuses on the errors made by previous models, improving upon them.\n",
    " - **Key Characteristics:**\n",
    "   - **Sequential Models**: The models are trained one after the other, with each new model correcting the mistakes of the previous ones.\n",
    "   - **Bias Reduction**: Boosting is primarily used to reduce bias by giving more weight to incorrectly classified data points.\n",
    " - **Common Example**: AdaBoost (Adaptive Boosting) is a popular boosting algorithm that adjusts the weights of incorrect predictions.\n",
    " - **Application**: Boosting is best suited for improving the performance of weak learners by combining them into a stronger, more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558a0b1",
   "metadata": {},
   "source": [
    "## 3. Stacking\n",
    " - **Definition**: Stacking involves training multiple models and combining their outputs to create a new dataset, which is then used by another model to make predictions.\n",
    " - **Key Characteristics:**\n",
    "   - **Model Combination**: The outputs from several base models are used to form a new dataset, and another model is trained on this dataset to make the final prediction.\n",
    "   - **Improved Performance:** By combining the outputs from different models, stacking often results in improved accuracy and robustness.\n",
    " - **Common Example:** A stacking model may combine outputs from decision trees, support vector machines (SVM), and logistic regression, with a final estimator like Logistic Regression or Random Forest to make the final prediction.\n",
    " - **Application:** Stacking is used when you want to combine the strengths of different models and improve prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a58fe",
   "metadata": {},
   "source": [
    "# What is a Random Forest?\n",
    "  - 1. **Ensemble of Decision Trees**\n",
    "  Random Forests combine multiple decision trees for classification or regression tasks.\n",
    "  - 2. **Robust Predictions**\n",
    "  By aggregation multiple trees, Random Forests reduce overfitting and improve generalization.\n",
    "\n",
    " - 3. **Versatile Application**\n",
    " Effective for various data types and problem domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a48e7a",
   "metadata": {},
   "source": [
    "## Random Forests Algorithm OR How Random Forest Works\n",
    " - **Creation**\n",
    " Ensemble learning techinque that creates multiple Decision Trees\n",
    " - **Variablity**\n",
    " Each tree uses random data and feature subsets to introduce variabilty\n",
    " - **Improved Performance**\n",
    " Reduces overfitting and improves prediction performance\n",
    " - **Result Aggregation**\n",
    " Aggregates results by voting (classification) or averaging(regression)\n",
    " - **Versatility**\n",
    " Handles complex data and provides reliable predictions across various domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2248ca03",
   "metadata": {},
   "source": [
    "## Adavantage of Random Forests\n",
    " - **High Accuracy**\n",
    " Combines multiple models for improved predictions\n",
    " - **Handles Missing Values**\n",
    " Robust to incomplete data\n",
    " - **Reduced Overfitting**\n",
    " Generalizes well to unseen data.\n",
    " - **Scalability**\n",
    " Efficient for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e33941",
   "metadata": {},
   "source": [
    "## Applications of Random Forest\n",
    " - **Finance**\n",
    " Credit sooring and risk assessment with robust predictions\n",
    " - **Environment**\n",
    " Monitoring land cover changes and preventing deforestation\n",
    " - **Cybersecurity**\n",
    " Fraud detection and anomaly spotting in online activities\n",
    " - **Medical Diagnosis**\n",
    " Predicting diseases based on patient data\n",
    " - **Fraud Detection**\n",
    " identifiying suspicious financial transactions.\n",
    " - **Stock market prediction**\n",
    " Forecasting market trends and stock prices \n",
    " - **Sustomer segmentation**\n",
    "  Grouping customers based on behaviour patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a81d14b",
   "metadata": {},
   "source": [
    "## Limitations of Random Forests\n",
    " 1. **Lack of interpretability**\n",
    " Comple model structure makes it difficult to explain individual predictions\n",
    "\n",
    " 2. **Slower predictions**\n",
    " Large forests can be computationally expensive during inference\n",
    "  \n",
    " 3. **Hyperparameter Sensitivity**\n",
    " Performance can vary significantly based on parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc6848",
   "metadata": {},
   "source": [
    "## Key Features of Random Forest\n",
    " 1. **High predictive Accuracy**\n",
    " Combine multiple decision trees for better predictions.\n",
    " 2. **Resistance to Overfitting**\n",
    " Reduces overfitting by averaging tree outputs\n",
    " 3. **Handles Large DAtasets**\n",
    " Processes large datasets efficiently by splitting data across tress.\n",
    " 4. **Feature Importance**\n",
    " Identifies key features influencing predictions.\n",
    " 5. **Built-in Cross-Validation**\n",
    " Uses out-o-bag samples for model validation\n",
    " 6. **Handles Missing VAlues**\n",
    " Adapts to missing data for robust predictions\n",
    " 7. **Parallelization**\n",
    " Build trees simultaneously for faster processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227705b1",
   "metadata": {},
   "source": [
    "## Random Forests vs. Decision Tree\n",
    "\n",
    "### Decision Trees\n",
    " - Single tree structure\n",
    " - Prone to overfitting\n",
    " - Less robust to noise \n",
    "\n",
    "### Random Forests\n",
    " - Ensemble of trees\n",
    " - Reducesd overfitting\n",
    " - More robust generalization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76600d0a",
   "metadata": {},
   "source": [
    "## Ensemble Learning Beyond Random Forests\n",
    "  - **Gradient Boosting**\n",
    "\n",
    "  Builds trees sequentially to correct previous errors. Ideal for complex relationships.\n",
    "  - **AdaBoost**\n",
    "\n",
    "  Adjusts weights of misclassified instances. Effective for binary classification\n",
    "  - **Stacking**\n",
    "  \n",
    "  Combines predictions from multiple models. Versatile for various problem types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "778a2c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd32b386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Dataset URL\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "\n",
    "# Column names as per dataset description\n",
    "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n",
    "           'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(url, header=None, names=columns)\n",
    "\n",
    "# Print basic info\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38dafe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for missing values:\n",
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "Insulin                     0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# step 2: Handle Missing Value\n",
    "print(\"\\nChecking for missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# fill missing numerical values with the median\n",
    "df.fillna(df.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# fill missing categorical values with the mode if any\n",
    "for col in df.select_dtypes(include=['object']):\n",
    "    df[col].fillna(df[col].mode()[0], implace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96b32765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "# step 3: prepare data \n",
    "# separate features and target vaiable\n",
    "X = df.drop(columns=['Outcome'])\n",
    "y = df['Outcome']\n",
    "\n",
    "# step 4: apply standard scalling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# step 5: split data into training and testing sets\n",
    "X_train, X_test, y_trian, y_test = train_test_split(X_scaled, y,test_size=0.3, random_state=42 )\n",
    "\n",
    "#step 6: train a support vector machine classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# bagging model\n",
    "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=3)\n",
    "bagging_model.fit(X_train, y_trian)\n",
    "\n",
    "# prediction and accuracy\n",
    "y_pred_bag = bagging_model.predict(X_test)\n",
    "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred_bag))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a29503af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting Accuracy: 0.7489177489177489\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# boostin model\n",
    "boosting_model = AdaBoostClassifier(n_estimators=10)\n",
    "boosting_model.fit(X_train, y_trian)\n",
    "\n",
    "# prediction and accuracy\n",
    "y_pred_boost = boosting_model.predict(X_test)\n",
    "print(\"Boosting Accuracy:\", accuracy_score(y_test, y_pred_boost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "422959cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Accuracy: 0.7359307359307359\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# define base model\n",
    "base_model = [\n",
    "    ('tree', DecisionTreeClassifier()),\n",
    "    ('svm', SVC(probability=True))\n",
    "]\n",
    "\n",
    "# meta-model is logistic regression\n",
    "stacking_model = StackingClassifier(estimators=base_model, final_estimator=LogisticRegression())\n",
    "stacking_model.fit(X_train, y_trian)\n",
    "\n",
    "# prediction and accuracy\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "print(\"Stacking Accuracy:\", accuracy_score(y_test, y_pred_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bef6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
