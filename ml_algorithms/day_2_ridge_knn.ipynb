{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0230bef",
   "metadata": {},
   "source": [
    "## **Machine Learning Algorithm**\n",
    "\n",
    "## **Type**: Supervised Learning \n",
    "\n",
    "## **Regression + Classification**\n",
    "\n",
    "## **Day 2**: Ridge Regression + K-NN Algorithm\n",
    "\n",
    "## **Student**: Muhammad Shafiq\n",
    "\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ff209",
   "metadata": {},
   "source": [
    "## **What is Ridge Regression?**\n",
    "\n",
    "**Ridge Regression** is also known as **L2 Regularization** a  technique for analyzing multiple regression data that suffer from **multicollinearity**. It adds a penalty to the regression **coefficients to prevent overfitting**.\n",
    "Multicollinearity occurs when independent variable in regression are highly  correlated with each other.\n",
    "\n",
    "**Ridge regression** is a model-tuning method that is used to analyze any data that suffers from multicollinearity. This method performs **L2 regularization**. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values. \n",
    "\n",
    "**The cost function for ridge regression:**\n",
    "\n",
    "               Min(||Y – X(theta)||^2 + λ||theta||^2)\n",
    "\n",
    "**Lambda** is the penalty term. λ given here is denoted by an alpha parameter in the ridge function. So, by changing the values of alpha, we are controlling the penalty term. The higher the values of alpha, the bigger is the penalty and therefore the magnitude of coefficients is reduced.\n",
    "\n",
    "- It shrinks the parameters. Therefore, it is used to prevent multicollinearity\n",
    "- It reduces the model complexity by coefficient shrinkage\n",
    "\n",
    "\n",
    "### **Standardization** \n",
    "\n",
    "In ridge regression, the first step is to standardize the variables (both dependent and independent) by subtracting their means and dividing by their standard deviations. This causes a challenge in notation since we must somehow indicate whether the variables in a particular formula are standardized or not. As far as standardization is concerned, all ridge regression calculations are based on standardized variables. When the final regression coefficients are displayed, they are adjusted back into their original scale. However, the ridge trace is on a standardized scale.\n",
    "\n",
    "#### **Bias and variance trade-off**\n",
    "\n",
    "Bias and variance trade-off is generally complicated when it comes to building ridge regression models on an actual dataset. However, following the general trend which one needs to remember is:\n",
    "\n",
    "- The bias increases as λ increases.\n",
    "- The variance decreases as λ increases.\n",
    "\n",
    "\n",
    "Ridge regression introduces bias into the estimates to reduce their variance. The mean squared error (MSE) of the ridge estimator can be decomposed into bias and variance components:\n",
    "\n",
    "- **Bias**: Measures the error introduced by approximating a real-world problem, which may be complex, by a simplified model. In ridge regression, as the regularization parameter k increases, the model becomes simpler, which increases bias but reduces variance.\n",
    "\n",
    "- **Variance**: Measures how much the ridge regression model's predictions would vary if we used different training data. As the regularization parameter k decreases, the model becomes more complex, fitting the training data more closely, which reduces bias but increases variance.\n",
    "\n",
    "- **Irreducible Error**: Represents the noise in the data that cannot be reduced by any model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc86e1e",
   "metadata": {},
   "source": [
    "## **Selection of the Ridge Parameter in Ridge Regression**\n",
    "\n",
    "Choosing an appropriate value for the ridge parameter k is crucial in ridge regression, as it directly influences the bias-variance tradeoff and the overall performance of the model. Several methods have been proposed for selecting the optimal ridge parameter, each with its own advantages and limitations. Methods for Selecting the Ridge Parameter are:\n",
    "\n",
    "#### **1. Cross-Validation**\n",
    "\n",
    "Cross-validation is a common method for selecting the ridge parameter by dividing data into subsets. The model trains on some subsets and validates on others, repeating this process and averaging the results to find the optimal value of k.\n",
    "\n",
    "- **K-Fold Cross-Validation**: The data is split into K subsets, training on K-1 folds and validating on the remaining fold. This is repeated K times, with each fold serving as the validation set once.\n",
    "\n",
    "- **Leave-One-Out Cross-Validation (LOOCV)** A special case of K-fold where K equals the number of observations, training on all but one observation and validating on the remaining one. It’s computationally intensive but unbiased.\n",
    "\n",
    "#### **2. Generalized Cross-Validation (GCV)**\n",
    "\n",
    "Generalized Cross-Validation is an extension of cross-validation that provides a more efficient way to estimate the optimal k without explicitly dividing the data. GCV is based on the idea of minimizing a function that approximates the leave-one-out cross-validation error. It is computationally less intensive and often yields similar results to traditional cross-validation methods.\n",
    "\n",
    "#### **3. Information Criteria**\n",
    "\n",
    "Information criteria such as the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) can also be used to select the ridge parameter. These criteria balance the goodness of fit of the model with its complexity, penalizing models with more parameters.\n",
    "\n",
    "#### **4. Empirical Bayes Methods**\n",
    "\n",
    "Empirical Bayes methods involve estimating the ridge parameter by treating it as a hyperparameter in a Bayesian framework. These methods use prior distributions and observed data to estimate the posterior distribution of the ridge parameter.\n",
    "\n",
    "- **Empirical Bayes Estimation**: This method involves specifying a prior distribution for k and using the observed data to update this prior to obtain a posterior distribution. The mode or mean of the posterior distribution is then used as the estimate of k.\n",
    "\n",
    "#### **5. Stability Selection**\n",
    "Stability selection improves ridge parameter robustness by subsampling data and fitting the model multiple times. The most frequently selected parameter across all subsamples is chosen as the final estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed53e58",
   "metadata": {},
   "source": [
    "#### **Applications of Ridge Regression**\n",
    "\n",
    "- **Forecasting Economic Indicators**: Ridge regression helps predict economic factors like GDP, inflation, and unemployment by managing multicollinearity between predictors like interest rates and consumer spending, leading to more accurate forecasts.\n",
    "\n",
    "- **Medical Diagnosis**: In healthcare, it aids in building diagnostic models by controlling multicollinearity among biomarkers, improving disease diagnosis and prognosis.\n",
    "\n",
    "- **Sales Prediction**: In marketing, ridge regression forecasts sales based on factors like advertisement costs and promotions, handling correlations between these variables for better sales planning.\n",
    "\n",
    "- **Climate Modeling**: Ridge regression improves climate models by eliminating interference between variables like temperature and precipitation, ensuring more accurate predictions.\n",
    "\n",
    "- **Risk Management**: In credit scoring and financial risk analysis, ridge regression evaluates creditworthiness by addressing multicollinearity among financial ratios, enhancing accuracy in risk management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba0d7d",
   "metadata": {},
   "source": [
    "### **Advantages and Disadvantages of Ridge Regression**\n",
    "\n",
    "##### **Advantages:**\n",
    "\n",
    "- **Stability**: Ridge regression provides more stable estimates in the presence of multicollinearity.\n",
    "\n",
    "- **Bias-Variance Tradeoff**: By introducing bias, ridge regression reduces the variance of the estimates, leading to lower MSE.\n",
    "\n",
    "- **Interpretability**: Unlike principal component regression, ridge regression retains the original predictors, making the results easier to interpret.\n",
    "\n",
    "##### **Disadvantages**:\n",
    "\n",
    "- **Bias Introduction**: The introduction of bias can lead to underestimation of the true effects of the predictors.\n",
    "\n",
    "- **Parameter Selection**: Choosing the optimal ridge parameter k can be challenging and computationally intensive.\n",
    "\n",
    "- **Not Suitable for Variable Selection**: Ridge regression does not perform variable selection, meaning all predictors remain in the model, even those with negligible effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3838fa1",
   "metadata": {},
   "source": [
    "**Read From Here For Full Details of Redge Regression**\n",
    "\n",
    "[Machine Learning Ridge Regression](https://www.geeksforgeeks.org/machine-learning/what-is-ridge-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569bb6b4",
   "metadata": {},
   "source": [
    "### **Redge Regression on Diabetes data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "300b6a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01, R2 Score: 0.487, MSE: 2836.41\n",
      "Alpha: 0.1, R2 Score: 0.492, MSE: 2810.04\n",
      "Alpha: 1, R2 Score: 0.438, MSE: 3105.47\n",
      "Alpha: 10, R2 Score: 0.156, MSE: 4664.72\n",
      "Alpha: 100, R2 Score: 0.009, MSE: 5479.45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "alphas = [0.01, 0.1, 1, 10, 100]\n",
    "for alpha in alphas:\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Alpha: {alpha}, R2 Score: {r2_score(y_test, y_pred):.3f}, MSE: {mean_squared_error(y_test, y_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dffa77",
   "metadata": {},
   "source": [
    "### **Redge Regression on california housing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9fecb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.5758185345441319\n",
      "MSE: 0.5558512007367515\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 2. Optional: scale features (important for regularized models)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Train Ridge model\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 6. Evaluate\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f5e8c",
   "metadata": {},
   "source": [
    "| Metric   | Meaning                                         |\n",
    "| -------- | ----------------------------------------------- |\n",
    "| R² Score | % of variance explained (closer to 1 is better) |\n",
    "| MSE      | Mean squared error (closer to 0 is better)      |\n",
    "\n",
    "\n",
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31d9f8",
   "metadata": {},
   "source": [
    "# **Topic 2: KNN(K-Nearst Neighbours) Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d130c",
   "metadata": {},
   "source": [
    "## **K-Nearest Neighbor(KNN) Algorithm**\n",
    "\n",
    "K-Nearst Neighbors (KNN) is a supervised machine learning algorithm generally used for classification but can also be used for regression tasks. It works by finding the \"k\" closest data points (neighbors) to a given input and makes a predictions based on the majority class (for classificaiton) or the averages value of (for regression). \n",
    "\n",
    "K-Nearest Neighbors is also called as a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification it performs an action on the dataset.\n",
    "\n",
    "\n",
    "### **What is 'K' in K Nearest Neighbour?**\n",
    "\n",
    "In the k-Nearst Neighbours algorithm k is just a number that tells the algorithm how many nearby points or neighbors to look when it makes a decision.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Imagine you're deciding which fruit it is based on its shape and size. you compare it to fruits you already know.\n",
    "\n",
    "- if K=3 the algorithm looks at the 3 closest fruits to the new one.\n",
    "\n",
    "- if 2 of those 3 fruits are apples and 1 is banna, the algorithm say the new fruit is an apple because most of its neighbors are apples.\n",
    "\n",
    "### **How to choose the value of k for KNN Algorithm?**\n",
    "\n",
    "- The value of k in KNN decides how many neighbors the algorithm looks at when making a prediction.\n",
    "\n",
    "- Choosing the right k is important for good results.\n",
    "\n",
    "- If the data has lots of noise or outliers, using a larger k can make the predictions more stable.\n",
    "\n",
    "- But if k is too large the model may become too simple and miss important patterns and this is called underfitting.\n",
    "\n",
    "- So k should be picked carefully based on the data.\n",
    "\n",
    "## **Statistical Methods for Selecting k**\n",
    "\n",
    "- **Cross-Validation**: Cross-Validation is a good way to find the best value of k is by using k-fold cross-validation. This means dividing the dataset into k parts. The model is trained on some of these parts and tested on the remaining ones. This process is repeated for each part. The k value that gives the highest average accuracy during these tests is usually the best one to use.\n",
    "\n",
    "- **Elbow Method**: In Elbow Method we draw a graph showing the error rate or accuracy for different k values. As k increases the error usually drops at first. But after a certain point error stops decreasing quickly. The point where the curve changes direction and looks like an \"elbow\" is usually the best choice for k.\n",
    "\n",
    "- **Odd Values for k**: It’s a good idea to use an odd number for k especially in classification problems. This helps avoid ties when deciding which class is the most common among the neighbors.\n",
    "\n",
    "\n",
    "## **Distance Metrics Used in KNN Algorithm**\n",
    "\n",
    "KNN uses distance metrics to identify nearest neighbor, these neighbors are used for classification and regression task. To identify nearest neighbor we use below distance metrics:\n",
    "\n",
    "### **1. Euclidean Distance**\n",
    "\n",
    "Euclidean distance is defined as the straight-line distance between two points in a plane or space. You can think of it like the shortest path you would walk if you were to go directly from one point to another.\n",
    "\n",
    "             distance(x,Xi) = ∑j=1d(xj−Xij)2]\n",
    "\n",
    "\n",
    "\n",
    "### **2. Manhattan Distance**\n",
    "\n",
    "This is the total distance you would travel if you could only move along horizontal and vertical lines like a grid or city streets. It’s also called \"taxicab distance\" because a taxi can only drive along the grid-like streets of a city.\n",
    "\n",
    "                    d(x,y)=∑i=1n∣xi−yi∣\n",
    "\n",
    "\n",
    "### **3. Minkowski Distance**\n",
    "\n",
    "Minkowski distance is like a family of distances, which includes both Euclidean and Manhattan distances as special cases.\n",
    "\n",
    "                   d(x,y)=(∑i=1n(xi−yi)p)1p\n",
    "\n",
    " \n",
    "From the formula above, when p=2, it becomes the same as the Euclidean distance formula and when p=1, it turns into the Manhattan distance formula. Minkowski distance is essentially a flexible formula that can represent either Euclidean or Manhattan distance depending on the value of p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2347d9f",
   "metadata": {},
   "source": [
    "## **Working of KNN algorithm**\n",
    "\n",
    "Thе K-Nearest Neighbors (KNN) algorithm operates on the principle of similarity where it predicts the label or value of a new data point by considering the labels or values of its K nearest neighbors in the training dataset.\n",
    "\n",
    "### **Step 1: Selecting the optimal value of K**\n",
    "\n",
    "- K represents the number of nearest neighbors that needs to be considered while making prediction.\n",
    "\n",
    "### **Step 2: Calculating distance**\n",
    "\n",
    "- To measure the similarity between target and training data points Euclidean distance is used. Distance is calculated between data points in the dataset and target point.\n",
    "\n",
    "### **Step 3: Finding Nearest Neighbors**\n",
    "\n",
    "- The k data points with the smallest distances to the target point are nearest neighbors.\n",
    "\n",
    "### **Step 4: Voting for Classification or Taking Average for Regression**\n",
    "\n",
    "- When you want to classify a data point into a category like spam or not spam, the KNN algorithm looks at the K closest points in the dataset. These closest points are called neighbors. The algorithm then looks at which category the neighbors belong to and picks the one that appears the most. This is called majority voting.\n",
    "\n",
    "- In regression, the algorithm still looks for the K closest points. But instead of voting for a class in classification, it takes the average of the values of those K neighbors. This average is the predicted value for the new point for the algorithm.\n",
    "\n",
    "It shows how a test point is classified based on its nearest neighbors. As the test point moves the algorithm identifies the closest 'k' data points i.e. 5 in this case and assigns test point the majority class label that is grey label class here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f8056",
   "metadata": {},
   "source": [
    "## **Applications of KNN**\n",
    "\n",
    "- **Recommendation Systems**: Suggests items like movies or products by finding users with similar preferences.\n",
    "- **Spam Detection**: Identifies spam emails by comparing new emails to known spam and non-spam examples.\n",
    "- **Customer Segmentation**: Groups customers by comparing their shopping behavior to others.\n",
    "- **Speech Recognition**: Matches spoken words to known patterns to convert them into text.\n",
    "## **Advantages of KNN**\n",
    "- **Simple to use**: Easy to understand and implement.\n",
    "- **No training step**: No need to train as it just stores the data and uses it during prediction.\n",
    "- **Few parameters**: Only needs to set the number of neighbors (k) and a distance method.\n",
    "- **Versatile**: Works for both classification and regression problems.\n",
    "## **Disadvantages of KNN**\n",
    "- **Slow with large data**: Needs to compare every point during prediction.\n",
    "- **Struggles with many features**: Accuracy drops when data has too many features.\n",
    "- **Can Overfit**: It can overfit especially when the data is high-dimensional or not clean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7b671",
   "metadata": {},
   "source": [
    "## **KNN Classifier with Breast Cancer dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967343cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9473684210526315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1. Load dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 2. Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Train KNN\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 6. Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b366cb",
   "metadata": {},
   "source": [
    "## **important parameter of `KNeighborsClassifier`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0a6ec1",
   "metadata": {},
   "source": [
    "| Parameter     | Description                                      |\n",
    "| ------------- | ------------------------------------------------ |\n",
    "| `n_neighbors` | Number of neighbors to use (default=5)           |\n",
    "| `weights`     | 'uniform' or 'distance' (closer = more weight)   |\n",
    "| `metric`      | Distance metric (`euclidean`, `manhattan`, etc.) |\n",
    "| `p`           | Power for Minkowski metric (p=2 = Euclidean)     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648895e",
   "metadata": {},
   "source": [
    "## **Try Different `K` values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "810affa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1 → Accuracy: 0.939\n",
      "K=2 → Accuracy: 0.939\n",
      "K=3 → Accuracy: 0.947\n",
      "K=4 → Accuracy: 0.956\n",
      "K=5 → Accuracy: 0.947\n",
      "K=6 → Accuracy: 0.947\n",
      "K=7 → Accuracy: 0.947\n",
      "K=8 → Accuracy: 0.956\n",
      "K=9 → Accuracy: 0.965\n",
      "K=10 → Accuracy: 0.956\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, 11):\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    acc = model.score(X_test, y_test)\n",
    "    print(f\"K={k} → Accuracy: {acc:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_algorithm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
