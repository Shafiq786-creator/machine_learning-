{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b3a839",
   "metadata": {},
   "source": [
    "## **Machine Learning Algorithm**\n",
    "\n",
    "## **Type**: Supervised Learning \n",
    "\n",
    "## **Regression + Classification**\n",
    "\n",
    "## **Day 1**: Linear Regression + Logistic Regression\n",
    "\n",
    "## **Student**: Muhammad Shafiq\n",
    "\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e012d6c",
   "metadata": {},
   "source": [
    "# **Topic 1: Linear Regression**\n",
    "\n",
    "**Linear regression** is a type of supervised machine-learning algorithm that learns from the labelled datasets and maps the data points with most optimized linear functions which can be used for prediction on new datasets. It assumes that there is a linear relationship between the input and output, meaning the output changes at a constant rate as the input changes. This relationship is represented by a straight line.\n",
    "\n",
    "**For example**:\n",
    "\n",
    "we want to predict a student's exam score based on how many hours they studied. We observe that as students study more hours, their scores go up. In the example of predicting exam scores based on hours studied. Here\n",
    "\n",
    "- **Independent variable (input)**: Hours studied because it's the factor we control or observe.\n",
    "\n",
    "- **Dependent variable (output)**: Exam score because it depends on how many hours were studied.\n",
    "\n",
    "#### **Equation**:\n",
    "\n",
    "                y^=β0 + β1x1 + β2x2 + ⋯ +βnxn + ϵ\n",
    "\n",
    "where \n",
    "\n",
    " - y^ = predicted Value\n",
    " - β = weights(learned from data) \n",
    " - x1,x2,xn = inputs features \n",
    " - ϵ : error/noise    \n",
    "\n",
    "### **Why Linear Regression is Important?**\n",
    "\n",
    "Here’s why linear regression is important:\n",
    "\n",
    "- **Simplicity and Interpretability**: It’s easy to understand and interpret, making it a starting point for learning about machine learning.\n",
    "\n",
    "- **Predictive Ability**: Helps predict future outcomes based on past data, making it useful in various fields like finance, healthcare and marketing.\n",
    "\n",
    "- **Basis for Other Models**: Many advanced algorithms, like logistic regression or neural networks, build on the concepts of linear regression.\n",
    "\n",
    "- **Efficiency**: It’s computationally efficient and works well for problems with a linear relationship.\n",
    "\n",
    "- **Widely Used**: It’s one of the most widely used techniques in both statistics and machine learning for regression tasks.\n",
    "\n",
    "- **Analysis**: It provides insights into relationships between variables (e.g., how much one variable influences another).            \n",
    "\n",
    "### **How Linear Regression Works**:\n",
    "\n",
    "                   min 1/n Σ(y^-y)^2\n",
    "\n",
    "Where \n",
    "\n",
    "  - y^ = predicted output\n",
    "  - y = actual output\n",
    "  - n = total number of observation\n",
    "  - min = indicate the goal to minimize the error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720850c6",
   "metadata": {},
   "source": [
    "## **Best Fit Line in Linear Regression**\n",
    "\n",
    "In linear regression, the **best-fit line** is the straight line that most accurately represents the relationship between the independent variable (input) and the dependent variable (output). It is the line that minimizes the difference between the actual data points and the predicted values from the model.\n",
    "\n",
    "\n",
    "### 1. **Goal of the Best-Fit Line**\n",
    "\n",
    "The goal of linear regression is to find a straight line that minimizes the error (the difference) between the observed data points and the predicted values. This line helps us predict the dependent variable for new, unseen data.\n",
    "\n",
    "### 2. **Equation of the Best-Fit Line**\n",
    "\n",
    "For simple linear regression (with one independent variable), the best-fit line is represented by the equation\n",
    "\n",
    "                              y = mx + b\n",
    "\n",
    "Where:\n",
    "\n",
    "- y is the predicted value (dependent variable)\n",
    "- x is the input (independent variable)\n",
    "- m is the slope of the line (how much y changes when x changes)\n",
    "- b is the intercept (the value of y when x = 0)\n",
    "\n",
    "The best-fit line will be the one that optimizes the values of m (slope) and b (intercept) so that the predicted y values are as close as possible to the actual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224bd30a",
   "metadata": {},
   "source": [
    "## **Implementation + Detail Explanation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f85b8f",
   "metadata": {},
   "source": [
    "#### **Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1283dfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5cf67f",
   "metadata": {},
   "source": [
    "## **Code Breakdown**\n",
    "\n",
    "### 1. **sklearn.linear_model**\n",
    "\n",
    "This module contains algorithms that assume a linear relationship between input features and output.\n",
    "\n",
    "**Popular Classes:**\n",
    "\n",
    "- **LinearRegression** → For regression tasks\n",
    "\n",
    "- **LogisticRegression** → For binary/multiclass classification\n",
    "\n",
    "- **Ridge, Lasso, ElasticNet** → Regularized regression variants\n",
    "\n",
    "- **SGDClassifier, SGDRegressor** → Linear models trained with stochastic gradient descent\n",
    "\n",
    "\n",
    "### 2. **sklearn.model_selection**\n",
    "\n",
    "This module provides functions for:\n",
    "\n",
    "- Splitting datasets\n",
    "\n",
    "- Cross-validation\n",
    "\n",
    "- Hyperparameter tuning (e.g., GridSearchCV, RandomizedSearchCV)\n",
    "\n",
    "\n",
    "**Popular Functions/Classes:**\n",
    "\n",
    "- **train_test_split()** → Splits dataset into train/test\n",
    "\n",
    "- **KFold, StratifiedKFold** → For cross-validation\n",
    "\n",
    "- **GridSearchCV, RandomizedSearchCV** → For tuning model parameters\n",
    "\n",
    "\n",
    "### **3. sklearn.datasets**\n",
    "\n",
    "This module provides preloaded toy datasets for learning and testing models.\n",
    "\n",
    "**Popular Functions:**\n",
    "\n",
    "- **load_diabetes()** → Regression dataset for predicting disease progression\n",
    "\n",
    "- **load_iris()** → Classification dataset with 3 flower types\n",
    "\n",
    "- **load_boston()** → House price regression (deprecated)\n",
    "\n",
    "- **load_digits(), load_wine(), load_breast_cancer()** → Others\n",
    "\n",
    "## **Othe Useful Modules**:\n",
    "\n",
    "\n",
    "| Module                  | Purpose                                                |\n",
    "| ----------------------- | ------------------------------------------------------ |\n",
    "| `sklearn.preprocessing` | Scaling, encoding, imputing missing values             |\n",
    "| `sklearn.metrics`       | Accuracy, precision, recall, F1, ROC, confusion matrix |\n",
    "| `sklearn.pipeline`      | Create ML pipelines to chain preprocessing and models  |\n",
    "| `sklearn.ensemble`      | Ensemble models like Random Forest, Gradient Boosting  |\n",
    "| `sklearn.svm`           | Support Vector Machines                                |\n",
    "| `sklearn.tree`          | Decision Trees                                         |\n",
    "| `sklearn.neighbors`     | KNN for classification and regression                  |\n",
    "| `sklearn.naive_bayes`   | Naive Bayes algorithms                                 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38356a36",
   "metadata": {},
   "source": [
    "### **Load DataSet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "962c1a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature [[ 0.03807591  0.05068012  0.06169621 ... -0.00259226  0.01990749\n",
      "  -0.01764613]\n",
      " [-0.00188202 -0.04464164 -0.05147406 ... -0.03949338 -0.06833155\n",
      "  -0.09220405]\n",
      " [ 0.08529891  0.05068012  0.04445121 ... -0.00259226  0.00286131\n",
      "  -0.02593034]\n",
      " ...\n",
      " [ 0.04170844  0.05068012 -0.01590626 ... -0.01107952 -0.04688253\n",
      "   0.01549073]\n",
      " [-0.04547248 -0.04464164  0.03906215 ...  0.02655962  0.04452873\n",
      "  -0.02593034]\n",
      " [-0.04547248 -0.04464164 -0.0730303  ... -0.03949338 -0.00422151\n",
      "   0.00306441]]\n",
      "Output [151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n"
     ]
    }
   ],
   "source": [
    "diabetes_data = load_diabetes()\n",
    "\n",
    "# separate feature X and y\n",
    "X = diabetes_data.data   # All input features \n",
    "y = diabetes_data.target  # output (targetd values)\n",
    "\n",
    "print(f\"Input feature {X}\")\n",
    "print(f\"Output {y}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140dd63",
   "metadata": {},
   "source": [
    "## **Code Breakdown**\n",
    "\n",
    "- `load_diabetes()` returns a **Bunch object**, similar to a dictionary.\n",
    "\n",
    "- `data` → contains feature matrix (shape: [442, 10])\n",
    "\n",
    "- `target` → contains target values (disease progression)\n",
    "\n",
    "### What is banch object\n",
    "\n",
    "A bunch is like a dictionary with dot-notation access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f001f92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'DESCR', 'feature_names', 'data_filename', 'target_filename', 'data_module'])\n"
     ]
    }
   ],
   "source": [
    "print(diabetes_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527918f0",
   "metadata": {},
   "source": [
    "### **All Ways to load and extract X, y**\n",
    "\n",
    "| Method                                                     | Description                 |\n",
    "| ---------------------------------------------------------- | --------------------------- |\n",
    "| `X, y = load_diabetes(return_X_y=True)`                    | Quick one-liner             |\n",
    "| `bunch = load_diabetes(); X, y = bunch.data, bunch.target` | More flexible               |\n",
    "| `bunch[\"data\"]`, `bunch[\"target\"]`                         | Dictionary-style access     |\n",
    "| `pd.DataFrame(bunch.data)`                                 | Convert to pandas DataFrame |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8560d",
   "metadata": {},
   "source": [
    "### **Load as Pandas DataFrame (for EDA)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a2fc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
      "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
      "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
      "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
      "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
      "\n",
      "         s4        s5        s6  target  \n",
      "0 -0.002592  0.019907 -0.017646   151.0  \n",
      "1 -0.039493 -0.068332 -0.092204    75.0  \n",
      "2 -0.002592  0.002861 -0.025930   141.0  \n",
      "3  0.034309  0.022688 -0.009362   206.0  \n",
      "4 -0.002592 -0.031988 -0.046641   135.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "df[\"target\"] = diabetes_data.target  # Add target column\n",
    "\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb8fab",
   "metadata": {},
   "source": [
    "### **Splitting the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e43b62c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1396226e",
   "metadata": {},
   "source": [
    "### **Code Breakdown**\n",
    "\n",
    "| Variable        | Description                                    |\n",
    "| --------------- | ---------------------------------------------- |\n",
    "| `X`             | Feature matrix (input) — full dataset          |\n",
    "| `y`             | Target vector (output) — full dataset          |\n",
    "| `test_size=0.2` | 20% of data goes to test set, 80% to train set |\n",
    "| `X_train`       | Features for training (80% by default)         |\n",
    "| `X_test`        | Features for testing (20% by default)          |\n",
    "| `y_train`       | Labels for training                            |\n",
    "| `y_test`        | Labels for testing                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e16dff",
   "metadata": {},
   "source": [
    "### **Parameters of `train_test_split()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_test_split(*arrays, \n",
    "                 test_size=None, \n",
    "                 train_size=None, \n",
    "                 random_state=None, \n",
    "                 shuffle=True, \n",
    "                 stratify=None)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb3b6d",
   "metadata": {},
   "source": [
    "### 1. `test_size` (float or int)\n",
    "- Proportion (float like 0.2) → 20% test data\n",
    "\n",
    "- Count (int like 100) → 100 samples in test set\n",
    "\n",
    "❗ Can’t use with train_size in a way that both conflict.\n",
    "\n",
    "### 2. `train_size` (optional)\n",
    "- If test_size is 0.2 and you want exact train size → set train_size=0.8\n",
    "\n",
    "- Not mandatory if test_size is provided.\n",
    "\n",
    "### 3. `random_state` (int, optional)\n",
    "- Sets the random seed so split is reproducible\n",
    "\n",
    "- Use any int, like random_state=42 (common practice)\n",
    "\n",
    "### 4. `shuffle` (bool, default=True)\n",
    "- Shuffles the data before splitting\n",
    "\n",
    "- Set shuffle=False for time series or ordered data\n",
    "\n",
    "### 5. `stratify` (array-like, default=None)\n",
    "- Ensures same proportion of classes in train/test\n",
    "\n",
    "- Useful for imbalanced classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f9b0a3",
   "metadata": {},
   "source": [
    "## **Train Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "745294a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.43550586496261223\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Score\n",
    "print(\"R2 Score:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f00e6a",
   "metadata": {},
   "source": [
    "## **Code Breakdown**\n",
    "\n",
    "### 1. model = `LinearRegression()`\n",
    "\n",
    "Creates an instance of the LinearRegression model.\n",
    "\n",
    "This step:\n",
    "\n",
    "- **Initializes the model**\n",
    "\n",
    "- Sets up internal parameters (e.g. fit_intercept=True by default)\n",
    "\n",
    "- Model is not trained yet\n",
    "\n",
    "### 2. `model.fit(X_train, y_train)`\n",
    "\n",
    "This is where the training happens.\n",
    "\n",
    "It:\n",
    "\n",
    "- Computes the best coefficients (β values) for minimizing error\n",
    "\n",
    "- Learns the mapping between X_train (input) and y_train (output)\n",
    "\n",
    "- Stores those learned weights inside the model for prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7846c",
   "metadata": {},
   "source": [
    "## **Important Functions & Attributes of ML Models (e.g., `LinearRegression`)**\n",
    "\n",
    "| Type         | Name                   | Description                               |\n",
    "| ------------ | ---------------------- | ----------------------------------------- |\n",
    "|  Function  | `fit(X, y)`            | Train the model                           |\n",
    "|  Function  | `predict(X)`           | Predict output for new data               |\n",
    "|  Function  | `score(X, y)`          | Return R² score (for regression)          |\n",
    "|  Function  | `get_params()`         | Get model hyperparameters                 |\n",
    "|  Function  | `set_params(**kwargs)` | Set model hyperparameters manually        |\n",
    "|  Attribute | `coef_`                | Array of learned weights (slopes)         |\n",
    "|  Attribute | `intercept_`           | Learned intercept (β₀)                    |\n",
    "|  Attribute | `n_iter_` *(for SGD)*  | Number of iterations used (if applicable) |\n",
    "|  Attribute | `rank_`, `singular_`   | Info about the X matrix (for analysis)    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05370694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 48.677449593771186\n",
      "MSE: 3425.115662804317\n",
      "R2 Score: 0.43550586496261223\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2a07e1",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854f197",
   "metadata": {},
   "source": [
    "# **Topic 2: LOGISTIC REGRESSION** (Classification)\n",
    "\n",
    "`Logistic Regression` is a supervised machine learning algorithm used for classification problems. Unlike linear regression which predicts continuous values it predicts the probability that an input belongs to a specific class. It is used for binary classification where the output can be one of two possible categories such as Yes/No, True/False or 0/1. It uses sigmoid function to convert inputs into a probability value between 0 and 1.\n",
    "\n",
    "#### **Types of Logistic Regression**\n",
    "\n",
    "Logistic regression can be classified into three main types based on the nature of the dependent variable:\n",
    "\n",
    "- **Binomial Logistic Regression**: This type is used when the dependent variable has only two possible categories. Examples include Yes/No, Pass/Fail or 0/1. It is the most common form of logistic regression and is used for binary classification problems.\n",
    "\n",
    "- **Multinomial Logistic Regression**: This is used when the dependent variable has three or more possible categories that are not ordered. For example, classifying animals into categories like \"cat,\" \"dog\" or \"sheep.\" It extends the binary logistic regression to handle multiple classes.\n",
    "\n",
    "- **Ordinal Logistic Regression**: This type applies when the dependent variable has three or more categories with a natural order or ranking. Examples include ratings like \"low,\" \"medium\" and \"high.\" It takes the order of the categories into account when modeling.\n",
    "\n",
    "##  **Assumptions of Logistic Regression**\n",
    "\n",
    "Understanding the assumptions behind logistic regression is important to ensure the model is applied correctly, main assumptions are:\n",
    "\n",
    "- **Independent observations**: Each data point is assumed to be independent of the others means there should be no correlation or dependence between the input samples.\n",
    "\n",
    "- **Binary dependent variables**: It takes the assumption that the dependent variable must be binary, means it can take only two values. For more than two categories SoftMax functions are used.\n",
    "\n",
    "- **Linearity relationship between independent variables and log odds**: The model assumes a linear relationship between the independent variables and the log odds of the dependent variable which means the predictors affect the log odds in a linear way.\n",
    "\n",
    "- **No outliers**: The dataset should not contain extreme outliers as they can distort the estimation of the logistic regression coefficients.\n",
    "\n",
    "- **Large sample size**: It requires a sufficiently large sample size to produce reliable and stable results.\n",
    "\n",
    "## **Understanding Sigmoid Function**\n",
    "\n",
    " 1. The sigmoid function is a important part of logistic regression which is used to convert the raw output of the model into a probability value between 0 and 1.\n",
    "\n",
    " 2. This function takes any real number and maps it into the range 0 to 1 forming an \"S\" shaped curve called the sigmoid curve or logistic curve. Because probabilities must lie between 0 and 1, the sigmoid function is perfect for this purpose.\n",
    "\n",
    " 3. In logistic regression, we use a threshold value usually 0.5 to decide the class label.\n",
    "\n",
    " - If the sigmoid output is same or above the threshold, the input is classified as Class 1.\n",
    " - If it is below the threshold, the input is classified as Class 0.\n",
    " - This approach helps to transform continuous input values into meaningful class predictions.\n",
    "\n",
    "\n",
    "**Read From Here For Full Details of Logistic Regression**\n",
    "\n",
    "[Machine Learning Logistic Regression](https://www.geeksforgeeks.org/machine-learning/ml-linear-regression/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8298d76a",
   "metadata": {},
   "source": [
    "# **Use Cases**:\n",
    "- Spam detection\n",
    "\n",
    "- Customer churn prediction\n",
    "\n",
    "- Disease diagnosis (yes/no)\n",
    "\n",
    "- Fraud detection\n",
    "\n",
    "## **Strengths**:\n",
    "- Simple and fast\n",
    "\n",
    "- Outputs probabilities\n",
    "\n",
    "- Good baseline classifier\n",
    "\n",
    "###  **Limitations**:\n",
    "- Assumes linear boundary between classes\n",
    "\n",
    "- Struggles with non-linear relationships\n",
    "\n",
    "- Not great with high-dimensional or sparse data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9961c",
   "metadata": {},
   "source": [
    "## **Implementation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8eed5c",
   "metadata": {},
   "source": [
    "### **Import labraried**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbd2887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94312263",
   "metadata": {},
   "source": [
    "### **Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "376a3a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b74dfe7",
   "metadata": {},
   "source": [
    "### **Splitting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f4299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e8a1d",
   "metadata": {},
   "source": [
    "### **Train and predict from model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "233e85de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95        46\n",
      "           1       0.96      0.97      0.96        68\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hafiz Computer CC\\miniconda3\\envs\\ml_algorithm\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_algorithm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
