{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6add1f",
   "metadata": {},
   "source": [
    "## **Machine Learning Algorithm**\n",
    "\n",
    "## **Type**: Supervised Learning \n",
    "\n",
    "## **Regression + Classification**\n",
    "\n",
    "## **Day 3**: Lasso Regression + Decision Tree Algorithm\n",
    "\n",
    "## **Student**: Muhammad Shafiq\n",
    "\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86262c7",
   "metadata": {},
   "source": [
    "## **Lasso Regression:**\n",
    "\n",
    "Lasso Regression is a regression method based on **Least Absolute Shrinkage and Selection Operator** and is used in regression analysis for variable selection and regularization. It helps remove irrelevant data features and prevents overfitting. This allows features with weak influence to be clearly identified as the coefficients of less important variables are shrunk toward zero.\n",
    "\n",
    "Lasso Regression is a regularization technique used to prevent overfitting. It improves linear regression by adding a penalty term to the standard regression equation. It works by minimizing the sum of squared differences between the observed and predicted values by fitting a line to the data.\n",
    "\n",
    "However in real-world datasets features have strong correlations with each other known as multicollinearity where Lasso Regression actually helps.\n",
    "\n",
    "- **For example**: \n",
    "\n",
    "if we're predicting house prices based on features like location, square footage and number of bedrooms. Lasso Regression can identify most important features. It might determine that location and square footage are the key factors influencing price while others has less impact. By making coefficient for the bedroom feature to zero it simplifies the model and improves its accuracy.\n",
    "\n",
    "## **Understanding Lasso Regression Working**\n",
    "\n",
    "Lasso Regression is an extension of linear regression. While traditional linear regression minimizes the sum of squared differences between the observed and predicted values to find the best-fit line, it doesn’t handle the complexity of real-world data well when many factors are involved.\n",
    "\n",
    " 1. **Ordinary Least Squares (OLS) Regression**\n",
    "\n",
    " It builds on Ordinary Least Squares (OLS) Regression method by adding a penalty term. The basic equation for OLS is:\n",
    "\n",
    "                    min RSS=Σ(yᵢ− y^ᵢ)²\n",
    "\n",
    "Where\n",
    "\n",
    "yi  : is the observed value.\n",
    "y^ᵢ : is the predicted value for each data point \n",
    "\n",
    " 2. **Penalty Term for Lasso Regression**\n",
    "\n",
    " In Lasso regression a penalty term is added to the OLS equation. Penalty is the sum of the absolute values of the coefficients. Updated cost function becomes:\n",
    "\n",
    "\n",
    "                      RSS+λ×∑∣βi\n",
    "\n",
    "Where,\n",
    "\n",
    " βi:   represents the coefficients of the predictors\n",
    "\n",
    " λ : is the tuning parameter that controls the strength of the penalty. As λ increases more coefficients are pushed towards zero\n",
    "\n",
    " 3. **Shrinking Coefficients:**\n",
    "\n",
    " Key feature of Lasso is its ability to make coefficients of less important features to zero. This removes irrelevant features from the model helps in making it useful for high-dimensional data with many predictors relative to the number of observations.\n",
    "\n",
    "4. **Selecting the optimal λ**\n",
    "\n",
    " Selecting correct lambda value is important. Cross-validation techniques are used to find the optimal value helps in balancing model complexity and predictive performance.\n",
    "\n",
    " Primary objective of Lasso regression is to minimize residual sum of squares (RSS) along with a penalty term multiplied by the sum of the absolute values of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c7252",
   "metadata": {},
   "source": [
    "## **When to use Lasso Regression**\n",
    "\n",
    "Lasso Regression is useful in the following situations:\n",
    "\n",
    "- **Feature Selection**: It automatically selects most important features by reducing the coefficients of less significant features to zero.\n",
    "- **Collinearity**: When there is multicollinearity it can help us by reducing the coefficients of correlated variables and selecting only one of them.\n",
    "- **Regularization**: It helps preventing overfitting by penalizing large coefficients which is useful when the number of predictors is large.\n",
    "- **Interpretability**: Compared to traditional linear regression models that have all features lasso regression generates a model with fewer non-zero coefficients making model simpler to understand.\n",
    "\n",
    "## **Advantages of Lasso Regression**\n",
    "\n",
    "- **Feature Selection**: It removes the need to manually select most important features hence the developed regression model becomes simpler and more explainable.\n",
    "- **Regularization**: It constrains large coefficients so a less biased model is generated which is robust and general in its predictions.\n",
    "- **Interpretability**: This creates another models helps in making them simpler to understand and explain which is important in fields like healthcare and finance.\n",
    "- **Handles Large Feature Spaces**: It is effective in handling high-dimensional data such as images and videos.\n",
    "\n",
    "## **Disadvantages**\n",
    "\n",
    "- **Selection Bias**: Lasso may randomly select one variable from a group of highly correlated variables which leads to a biased model.\n",
    "- **Sensitive to Scale**: It is sensitive to features with different scales as they can impact the regularization and affect model's accuracy.\n",
    "- **Impact of Outliers**: It can be easily affected by the outliers in the given data which results to overfitting of the coefficients.\n",
    "- **Model Instability**: It can be unstable when there are many correlated variables which causes it to select different features with small changes in the data.\n",
    "- **Tuning Parameter Selection**: Analyzing different λ (alpha) values may be problematic but can be solved by cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1925954",
   "metadata": {},
   "source": [
    "**Read From Here For Full Details of Lasso Regression**\n",
    "\n",
    "[Machine Learning Lasso Regression](https://www.geeksforgeeks.org/machine-learning/what-is-lasso-regression/)\n",
    "\n",
    "[Machine Learning Lasso Regression](https://www.mygreatlearning.com/blog/understanding-of-lasso-regression/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f312c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.48658970856215966\n",
      "Non-zero coefficients: 9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Load and scale data\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Train Lasso model\n",
    "model = Lasso(alpha=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "print(\"Non-zero coefficients:\", (model.coef_ != 0).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da76583",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fad64",
   "metadata": {},
   "source": [
    "# **Decision Tree:**\n",
    "\n",
    "A Decision Tree helps us make decisions by showing different options and how they are related. It has a tree-like structure that starts with one main question called the root node which represents the entire dataset. From there, the tree branches out into different possibilities based on features in the data.\n",
    "\n",
    "- **Root Node**: Starting point representing the whole dataset.\n",
    "- **Branches**: Lines connecting nodes showing the flow from one decision to another.\n",
    "- **Internal Nodes**: Points where decisions are made based on data features.\n",
    "- **Leaf Nodes** : End points of the tree where the final decision or prediction is made.\n",
    "\n",
    "There are mainly two types of Decision Trees based on the target variable:\n",
    "\n",
    "- **Classification Trees**: Used for predicting categorical outcomes like spam or not spam. These trees split the data based on features to classify data into predefined categories.\n",
    "\n",
    "- **Regression Trees**: Used for predicting continuous outcomes like predicting house prices. Instead of assigning categories, it provides numerical predictions based on the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd8e7c7",
   "metadata": {},
   "source": [
    "## **Splitting Criteria in Decision Trees**\n",
    "\n",
    "In a Decision Tree, the process of splitting data at each node is important. The splitting criteria finds the best feature to split the data on. Common splitting criteria include Gini Impurity and Entropy.\n",
    "\n",
    "- **Gini Impurity**: This criterion measures how \"impure\" a node is. The lower the Gini Impurity the better the feature splits the data into distinct categories.\n",
    "- **Entropy**: This measures the amount of uncertainty or disorder in the data. The tree tries to reduce the entropy by splitting the data on features that provide the most information about the target variable.\n",
    "\n",
    "These criteria help decide which features are useful for making the best split at each decision point in the tree.\n",
    "\n",
    "### **Pruning in Decision Trees**\n",
    "\n",
    "Pruning is an important technique used to prevent overfitting in Decision Trees. Overfitting occurs when a tree becomes too deep and starts to memorize the training data rather than learning general patterns. This leads to poor performance on new, unseen data.\n",
    "This technique reduces the complexity of the tree by removing branches that have little predictive power. It improves model performance by helping the tree generalize better to new data. It also makes the model simpler and faster to deploy.\n",
    "It is useful when a Decision Tree is too deep and starts to capture noise in the data.\n",
    "\n",
    "### **Advantages of Decision Trees**\n",
    "\n",
    "- **Easy to Understand**: Decision Trees are visual which makes it easy to follow the decision-making process.\n",
    "- **Versatility**: Can be used for both classification and regression problems.\n",
    "- **No Need for Feature Scaling**: Unlike many machine learning models, it don’t require us to scale or normalize our data.\n",
    "- **Handles Non-linear Relationships**: It capture complex, non-linear relationships between features and outcomes effectively.\n",
    "- **Interpretability**: The tree structure is easy to interpret helps in allowing users to understand the reasoning behind each decision.\n",
    "- **Handles Missing Data**: It can handle missing values by using strategies like assigning the most common value or ignoring missing data during splits.\n",
    "\n",
    "## **Disadvantages of Decision Trees**\n",
    "\n",
    "- **Overfitting**: They can overfit the training data if they are too deep which means they memorize the data instead of learning general patterns. This leads to poor performance on unseen data.\n",
    "- **Instability**: It can be unstable which means that small changes in the data may lead to significant differences in the tree structure and predictions.\n",
    "- **Bias towards Features with Many Categories**: It can become biased toward features with many distinct values which focuses too much on them and potentially missing other important features which can reduce prediction accuracy.\n",
    "- **Difficulty in Capturing Complex Interactions**: Decision Trees may struggle to capture complex interactions between features which helps in making them less effective for certain types of data.\n",
    "- **Computationally Expensive for Large Datasets**: For large datasets, building and pruning a Decision Tree can be computationally intensive, especially as the tree depth increases.\n",
    "\n",
    "## **Applications of Decision Trees**\n",
    "\n",
    "Decision Trees are used across various fields due to their simplicity, interpretability and versatility lets see some key applications:\n",
    "\n",
    "- **Loan Approval in Banking**: Banks use Decision Trees to assess whether a loan application should be approved. The decision is based on factors like credit score, income, employment status and loan history. This helps predict approval or rejection helps in enabling quick and reliable decisions.\n",
    "- **Medical Diagnosis**: In healthcare they assist in diagnosing diseases. For example, they can predict whether a patient has diabetes based on clinical data like glucose levels, BMI and blood pressure. This helps classify patients into diabetic or non-diabetic categories, supporting early diagnosis and treatment.\n",
    "- **Predicting Exam Results in Education**: Educational institutions use to predict whether a student will pass or fail based on factors like attendance, study time and past grades. This helps teachers identify at-risk students and offer targeted support.\n",
    "- **Customer Churn Prediction**: Companies use Decision Trees to predict whether a customer will leave or stay based on behavior patterns, purchase history, and interactions. This allows businesses to take proactive steps to retain customers.\n",
    "- **Fraud Detection**: In finance, Decision Trees are used to detect fraudulent activities, such as credit card fraud. By analyzing past transaction data and patterns, Decision Trees can identify suspicious activities and flag them for further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74f543",
   "metadata": {},
   "source": [
    "**Read From Here For Full Details of Decision Tree Algorithm**\n",
    "\n",
    "[Machine Learning Decision Tree](https://www.geeksforgeeks.org/machine-learning/decision-tree/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_algorithm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
